{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5cef475-56eb-4c86-b78e-7ecd955856bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (from nltk) (4.63.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rusla\\miniconda3\\envs\\network-science\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0cb08514-fed9-4170-b6a4-3006725d4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import praw\n",
    "import praw.exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61f20a73-39d2-4e04-a182-d1b21d855872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IN HERE YOU CAN CHANGE YOUR DIRECTORY\n",
    "output_dir = 'data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = '9AkNcQ17Z5pi_zo36Qrr6g',\n",
    "    client_secret = 'bTQxJR7g2NVrYQZ1kNT1iipeMIGckA',\n",
    "    user_agent = 'Dry_Try8800',\n",
    "    check_for_async = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "447d287f-5c86-4ced-969a-d8ab7d641282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in vegan: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/veganSubs_vegan_posts.csv\n",
      "CSV saved successfully: data/veganSubs_vegan_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in eatcheapandvegan: 100%|██████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/veganSubs_eatcheapandvegan_posts.csv\n",
      "CSV saved successfully: data/veganSubs_eatcheapandvegan_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in veganrecipes: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/veganSubs_veganrecipes_posts.csv\n",
      "CSV saved successfully: data/veganSubs_veganrecipes_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in veganfitness: 100%|██████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/veganSubs_veganfitness_posts.csv\n",
      "CSV saved successfully: data/veganSubs_veganfitness_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in veganbaking: 100%|███████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/veganSubs_veganbaking_posts.csv\n",
      "CSV saved successfully: data/veganSubs_veganbaking_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in food: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_food_posts.csv\n",
      "CSV saved successfully: data/controlSubs_food_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in FoodPorn: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_FoodPorn_posts.csv\n",
      "CSV saved successfully: data/controlSubs_FoodPorn_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in foodhacks: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_foodhacks_posts.csv\n",
      "CSV saved successfully: data/controlSubs_foodhacks_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in nutrition: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_nutrition_posts.csv\n",
      "CSV saved successfully: data/controlSubs_nutrition_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in Cooking: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_Cooking_posts.csv\n",
      "CSV saved successfully: data/controlSubs_Cooking_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in recipes: 100%|███████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/controlSubs_recipes_posts.csv\n",
      "CSV saved successfully: data/controlSubs_recipes_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in meat: 100%|██████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/nonVeganSubs_meat_posts.csv\n",
      "CSV saved successfully: data/nonVeganSubs_meat_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in zerocarb: 100%|██████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/nonVeganSubs_zerocarb_posts.csv\n",
      "CSV saved successfully: data/nonVeganSubs_zerocarb_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in carnivore: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/nonVeganSubs_carnivore_posts.csv\n",
      "CSV saved successfully: data/nonVeganSubs_carnivore_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in carnivorediet: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/nonVeganSubs_carnivorediet_posts.csv\n",
      "CSV saved successfully: data/nonVeganSubs_carnivorediet_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reddit posts in antivegan: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved successfully: data/nonVeganSubs_antivegan_posts.csv\n",
      "CSV saved successfully: data/nonVeganSubs_antivegan_comments.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from scripts import scraper\n",
    "\n",
    "# Lists of subreddit categories\n",
    "veganSubs = [\"vegan\", \"eatcheapandvegan\", \"veganrecipes\", \"veganfitness\", \"veganbaking\"]\n",
    "controlSubs = [\"food\", \"FoodPorn\", \"foodhacks\", \"nutrition\", \"Cooking\", \"recipes\"]\n",
    "nonVeganSubs = [\"meat\", \"zerocarb\", \"carnivore\", \"carnivorediet\", \"antivegan\"]\n",
    "\n",
    "# Call the function for each category\n",
    "scraper.scrape_subreddits(veganSubs, \"veganSubs\", output_dir, posts_to_retrieve=1)\n",
    "scraper.scrape_subreddits(controlSubs, \"controlSubs\", output_dir, posts_to_retrieve=1)\n",
    "scraper.scrape_subreddits(nonVeganSubs, \"nonVeganSubs\", output_dir, posts_to_retrieve=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aff8ba31-466e-49e3-9e26-1bfdf6753e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              veganSubs  \\\n",
      "0     #**Welcome to r/EatCheapAndVegan.**\\n**Veganis...   \n",
      "1     Wow.. she is amazing! Givemy best regards to t...   \n",
      "2     Ok, you/she needs to post the recipe for that ...   \n",
      "3     She has an Instagram page (her handle is color...   \n",
      "4     Don’t mind me just figuring out how to steal y...   \n",
      "...                                                 ...   \n",
      "3580                                               None   \n",
      "3581                                               None   \n",
      "3582                                               None   \n",
      "3583                                               None   \n",
      "3584                                               None   \n",
      "\n",
      "                                           nonVeganSubs  \\\n",
      "0     Those are vegan lions, Instead of eating boars...   \n",
      "1     I was just explaining this on another sub wher...   \n",
      "2     This will never not be the funniest thing I’ve...   \n",
      "3     \"Hey Marty, thanks again for letting us eat yo...   \n",
      "4     We commend your scarifies for our children Mr ...   \n",
      "...                                                 ...   \n",
      "3580                                               None   \n",
      "3581                                               None   \n",
      "3582                                               None   \n",
      "3583                                               None   \n",
      "3584                                               None   \n",
      "\n",
      "                                            controlSubs  \n",
      "0     OP, I would be honored to cook your recipes an...  \n",
      "1     You're not a bother, thank you for sharing wit...  \n",
      "2     Thank you for sharing! I think I will try to m...  \n",
      "3     I lost my mother recently and now I'm cooking ...  \n",
      "4     u/TerrysApplianceSvc, \\n\\nI'm sorry about your...  \n",
      "...                                                 ...  \n",
      "3580  Applying that logic it would be OK to send out...  \n",
      "3581  Even from a restaurant perspective, this is a ...  \n",
      "3582  if the meat is chopped properly im sure it cou...  \n",
      "3583  Having the tail in the shell next to the pasta...  \n",
      "3584  The last thing you usually want to do with a l...  \n",
      "\n",
      "[3585 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "data = {'veganSubs': [], 'nonVeganSubs': [], 'controlSubs': []}\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir('data'):\n",
    "    if 'comments' in filename:\n",
    "        category = filename.split('_')[0] \n",
    "        df = pd.read_csv(os.path.join('data', filename))\n",
    "        data[category].extend(df['comment_body'].tolist())\n",
    "\n",
    "# Because we don't have same amount of comments in each post, we need to fill in the 'shorter'\n",
    "# columns with 'None' values in order to create a data frame. Because it requires all of the columns\n",
    "# to be the same length\n",
    "\n",
    "max_length = max(len(data['veganSubs']), len(data['nonVeganSubs']), len(data['controlSubs']))\n",
    "for category in data:\n",
    "    data[category].extend([None] * (max_length - len(data[category])))\n",
    "\n",
    "combined_df = pd.DataFrame(data)\n",
    "\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "92a147ab-b399-4f77-bde4-b469c7154ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "veganSubs_text = ' '.join(combined_df['veganSubs'].dropna())\n",
    "nonVeganSubs_text = ' '.join(combined_df['nonVeganSubs'].dropna())\n",
    "controlSubs_text = ' '.join(combined_df['controlSubs'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "30e10bed-4497-42c1-8fde-03f301394890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rusla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # We are lowering, removing any hiperlinks, removing any non-numerical strings\n",
    "    # tokenizing, removing stop words, and finally lemmatizing the text here\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Preprocess the combined comments\n",
    "vegan_processed = preprocess_text(veganSubs_text)\n",
    "nonVegan_processed = preprocess_text(nonVeganSubs_text)\n",
    "control_processed = preprocess_text(controlSubs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c4ae33dc-f6c9-4832-b770-f3366e06b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have preprocessed_comments as a dictionary\n",
    "preprocessed_comments = {\n",
    "    'veganSubs': vegan_processed,\n",
    "    'nonVeganSubs': nonVegan_processed,\n",
    "    'controlSubs': control_processed\n",
    "}\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_results = {}\n",
    "# Get the most important features (words)\n",
    "for category, text in preprocessed_comments.items():\n",
    "    # Fit and transform each category separately\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Store the results (you can analyze or print them here)\n",
    "    tfidf_results[category] = {\n",
    "        'matrix': tfidf_matrix,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "# Print top words for each category\n",
    "df_top = pd.DataFrame()\n",
    "for category, result in tfidf_results.items():\n",
    "    feature_array = result['matrix'].toarray().flatten()  # Convert sparse matrix to array\n",
    "    top_indices = feature_array.argsort()[-20:][::-1]  # Get indices of top 20 words\n",
    "    top_words = [result['feature_names'][i] for i in top_indices]\n",
    "    df_top[category] = top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5fbb61c5-0b18-4fc8-b6fd-2b8ac0343764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veganSubs</th>\n",
       "      <th>nonVeganSubs</th>\n",
       "      <th>controlSubs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>milk</td>\n",
       "      <td>im</td>\n",
       "      <td>recipe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vegan</td>\n",
       "      <td>meat</td>\n",
       "      <td>thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>starbucks</td>\n",
       "      <td>look</td>\n",
       "      <td>sharing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>thank</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>much</td>\n",
       "      <td>like</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>people</td>\n",
       "      <td>brisket</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>animal</td>\n",
       "      <td>dont</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dont</td>\n",
       "      <td>good</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>look</td>\n",
       "      <td>one</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>im</td>\n",
       "      <td>squeeze</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>thank</td>\n",
       "      <td>know</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>israel</td>\n",
       "      <td>eat</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>would</td>\n",
       "      <td>get</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>make</td>\n",
       "      <td>would</td>\n",
       "      <td>try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>also</td>\n",
       "      <td>thats</td>\n",
       "      <td>ill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>get</td>\n",
       "      <td>carnivore</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>one</td>\n",
       "      <td>go</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>way</td>\n",
       "      <td>youre</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cost</td>\n",
       "      <td>fat</td>\n",
       "      <td>would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>good</td>\n",
       "      <td>make</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    veganSubs nonVeganSubs controlSubs\n",
       "0        milk           im      recipe\n",
       "1       vegan         meat       thank\n",
       "2   starbucks         look     sharing\n",
       "3        like        thank          im\n",
       "4        much         like        make\n",
       "5      people      brisket        love\n",
       "6      animal         dont         one\n",
       "7        dont         good        like\n",
       "8        look          one        time\n",
       "9          im      squeeze        food\n",
       "10      thank         know        good\n",
       "11     israel          eat        cook\n",
       "12      would          get        much\n",
       "13       make        would         try\n",
       "14       also        thats         ill\n",
       "15        get    carnivore      family\n",
       "16        one           go        look\n",
       "17        way        youre        rice\n",
       "18       cost          fat       would\n",
       "19       good         make         get"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a27e73-f56a-4d39-b851-3a17354db328",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
