{"cells":[{"cell_type":"code","execution_count":1,"id":"0cb08514-fed9-4170-b6a4-3006725d4876","metadata":{"id":"0cb08514-fed9-4170-b6a4-3006725d4876","executionInfo":{"status":"ok","timestamp":1739026062693,"user_tz":-60,"elapsed":2156,"user":{"displayName":"Ruslan Nuriyev","userId":"06841590792812019569"}}},"outputs":[],"source":["import pandas as pd\n","import polars as pl\n","import time\n","import os\n","from tqdm.auto import tqdm"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"qWDxOMVcUF64"},"id":"qWDxOMVcUF64","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aff8ba31-466e-49e3-9e26-1bfdf6753e7b","metadata":{"id":"aff8ba31-466e-49e3-9e26-1bfdf6753e7b"},"outputs":[],"source":["data = {'veganSubs': [], 'nonVeganSubs': [], 'controlSubs': []}\n","data_path = '/content/drive/MyDrive/Network Science project/reddit_data'\n","\n","\n","# Loop through all files in the directory\n","for filename in os.listdir(data_path):\n","    if any(x in data.keys() for x in filename.split('_')) and ('comments' in filename):\n","        category = filename.split('_')[0]\n","        df = pl.read_csv(os.path.join(data_path, filename))\n","        data[category].extend(df['comment_body'].to_list())\n","\n","# Because we don't have same amount of comments in each post, we need to fill in the 'shorter'\n","# columns with 'None' values in order to create a data frame. Because it requires all of the columns\n","# to be the same length\n","\n","max_length = max(len(data['veganSubs']), len(data['nonVeganSubs']), len(data['controlSubs']))\n","for category in data:\n","    data[category].extend([None] * (max_length - len(data[category])))\n","\n","combined_df = pl.DataFrame(data)\n","\n","print(combined_df)"]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"5yI7EmnjgHu7"},"id":"5yI7EmnjgHu7","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"92a147ab-b399-4f77-bde4-b469c7154ecd","metadata":{"id":"92a147ab-b399-4f77-bde4-b469c7154ecd"},"outputs":[],"source":["veganSubs_list = combined_df['veganSubs'].drop_nulls()\n","nonVeganSubs_list = combined_df['nonVeganSubs'].drop_nulls()\n","controlSubs_list = combined_df['controlSubs'].drop_nulls()"]},{"cell_type":"code","execution_count":null,"id":"30e10bed-4497-42c1-8fde-03f301394890","metadata":{"id":"30e10bed-4497-42c1-8fde-03f301394890"},"outputs":[],"source":["import re\n","import spacy\n","\n","nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n","\n","# Define a preprocessing function\n","def clean_text(text):\n","    # Lowercase\n","    text = text.lower()\n","    # Remove hyperlinks\n","    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n","    # Remove non-alphanumeric characters except spaces\n","    text = re.sub(r'[^a-z0-9\\s]', '', text)\n","    return text.strip()\n","\n","def lemmatize_texts(texts, batch_size=50):\n","    lemmatized_texts = []\n","    for doc in nlp.pipe(texts, batch_size=batch_size):\n","        lemmatized_texts.append(\" \".join([token.lemma_ for token in doc if not token.is_stop]))\n","    return lemmatized_texts\n","\n","\n","\n","# Preprocess the combined comments\n","vegan_processed = veganSubs_list.map_elements(clean_text)\n","nonVegan_processed = nonVeganSubs_list.map_elements(clean_text)\n","control_processed = controlSubs_list.map_elements(clean_text)\n","\n","vegan_processed = lemmatize_texts(vegan_processed)\n","nonVegan_processed = lemmatize_texts(nonVegan_processed)\n","control_processed = lemmatize_texts(control_processed)"]},{"cell_type":"code","execution_count":null,"id":"0ec8ba44-aa32-43f0-b61a-b5a3c1fb51fa","metadata":{"id":"0ec8ba44-aa32-43f0-b61a-b5a3c1fb51fa"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vegan_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","nonvegan_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","control_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","\n","vegan_matrix = vegan_tfidf.fit_transform(vegan_processed)\n","vegan_features = vegan_tfidf.get_feature_names_out()\n","\n","nonvegan_matrix = nonvegan_tfidf.fit_transform(nonVegan_processed)\n","nonvegan_features = nonvegan_tfidf.get_feature_names_out()\n","\n","control_matrix = control_tfidf.fit_transform(control_processed)\n","control_features = control_tfidf.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"id":"91416331-86d8-4677-b86d-9b678776a490","metadata":{"id":"91416331-86d8-4677-b86d-9b678776a490"},"outputs":[],"source":["vegan_scores = vegan_matrix.mean(axis=0).argsort()\n","vegan_top20 = vegan_features[vegan_scores[::-1]][0][:20]\n","\n","nonvegan_scores = nonvegan_matrix.mean(axis=0).argsort()\n","nonvegan_top20 = nonvegan_features[nonvegan_scores[::-1]][0][:20]\n","\n","control_scores = control_matrix.mean(axis=0).argsort()\n","control_top20 = control_features[control_scores[::-1]][0][:20]"]},{"cell_type":"code","execution_count":null,"id":"343a68d7-966a-4b67-b29e-4e0e73c5117b","metadata":{"id":"343a68d7-966a-4b67-b29e-4e0e73c5117b"},"outputs":[],"source":["top20_df = pd.DataFrame({'vegan': vegan_top20,\n","                         'nonVegan': nonvegan_top20,\n","                         'control': control_top20})"]},{"cell_type":"code","execution_count":null,"id":"d045e463-dfd0-4923-8aa2-05449fe932cf","metadata":{"id":"d045e463-dfd0-4923-8aa2-05449fe932cf"},"outputs":[],"source":["top20_df"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# **Grouped Data**\n","\n"],"metadata":{"id":"Zpcd27Ojztar"},"id":"Zpcd27Ojztar"},{"cell_type":"code","source":["import os\n","import polars as pl\n","\n","# Initialize empty DataFrames for each category\n","combined_dfs = {\n","    \"veganSubs\": pl.DataFrame(),\n","    \"nonVeganSubs\": pl.DataFrame(),\n","    \"controlSubs\": pl.DataFrame()\n","}\n","\n","# Iterate through files in the directory\n","for filename in os.listdir(data_path):\n","    for category in combined_dfs.keys():\n","        if category in filename and \"comments\" in filename:\n","            df = pl.read_csv(os.path.join(data_path, filename))\n","\n","            if \"post_id\" in df.columns and \"comment_body\" in df.columns:\n","                combined_dfs[category] = pl.concat(\n","                    [combined_dfs[category], df.select([\"post_id\", \"comment_body\"])],\n","                    how=\"vertical\"\n","                )\n","\n","# Access the combined DataFrames\n","vegan_df = combined_dfs[\"veganSubs\"]\n","non_vegan_df = combined_dfs[\"nonVeganSubs\"]\n","control_df = combined_dfs[\"controlSubs\"]"],"metadata":{"id":"aCKvyiT4yq9f"},"id":"aCKvyiT4yq9f","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vegan_grouped = vegan_df.group_by('post_id').agg(pl.col('comment_body').str.join(' '))['comment_body']\n","nonvegan_grouped = non_vegan_df.group_by('post_id').agg(pl.col('comment_body').str.join(' '))['comment_body']\n","control_grouped = control_df.group_by('post_id').agg(pl.col('comment_body').str.join(' '))['comment_body']"],"metadata":{"id":"5tMVTKNy9t-F"},"id":"5tMVTKNy9t-F","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess the combined comments\n","vegan_processed_grouped = vegan_grouped.map_elements(clean_text)\n","nonVegan_processed_grouped = nonvegan_grouped.map_elements(clean_text)\n","control_processed_grouped = control_grouped.map_elements(clean_text)\n","\n","spacy.require_gpu()\n","nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n","nlp.max_length = 1200000\n","\n","vegan_processed_grouped = lemmatize_texts(vegan_processed_grouped)\n","nonVegan_processed_grouped = lemmatize_texts(nonVegan_processed_grouped)\n","control_processed_grouped = lemmatize_texts(control_processed_grouped)"],"metadata":{"id":"D26aAsID9lPx"},"id":"D26aAsID9lPx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vegan_grouped_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","nonvegan_grouped_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","control_grouped_tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.5)\n","\n","vegan_grouped_matrix = vegan_grouped_tfidf.fit_transform(vegan_processed_grouped)\n","vegan_grouped_features = vegan_grouped_tfidf.get_feature_names_out()\n","\n","nonvegan_grouped_matrix = nonvegan_grouped_tfidf.fit_transform(nonVegan_processed_grouped)\n","nonvegan_grouped_features = nonvegan_grouped_tfidf.get_feature_names_out()\n","\n","control_grouped_matrix = control_grouped_tfidf.fit_transform(control_processed_grouped)\n","control_grouped_features = control_grouped_tfidf.get_feature_names_out()"],"metadata":{"id":"LFlwt1pNA2LC"},"id":"LFlwt1pNA2LC","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vegan_grouped_scores = vegan_grouped_matrix.mean(axis=0).argsort()\n","vegan_grouped_top20 = vegan_grouped_features[vegan_grouped_scores[::-1]][0][:20]\n","\n","nonvegan_grouped_scores = nonvegan_grouped_matrix.mean(axis=0).argsort()\n","nonvegan_grouped_top20 = nonvegan_grouped_features[nonvegan_grouped_scores[::-1]][0][:20]\n","\n","control_grouped_scores = control_grouped_matrix.mean(axis=0).argsort()\n","control_grouped_top20 = control_grouped_features[control_grouped_scores[::-1]][0][:20]"],"metadata":{"id":"C4UzgARjA7sG"},"id":"C4UzgARjA7sG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["top20_grouped = pd.DataFrame({'vegan': vegan_grouped_top20,\n","                         'nonVegan': nonvegan_grouped_top20,\n","                         'control': control_grouped_top20})"],"metadata":{"id":"X_OVA83HA8_t"},"id":"X_OVA83HA8_t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["top20_grouped"],"metadata":{"id":"-YKJ2fHuBvxo"},"id":"-YKJ2fHuBvxo","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Network"],"metadata":{"id":"XCiKHewgz8iy"},"id":"XCiKHewgz8iy"},{"cell_type":"code","source":["tokenized_vegan_grouped = [comment.split() for comment in vegan_processed_grouped]\n","tokenized_nonvegan_grouped = [comment.split() for comment in nonVegan_processed_grouped]\n","tokenized_control_grouped = [comment.split() for comment in control_processed_grouped]"],"metadata":{"id":"HuBwKbf-ZzOf"},"id":"HuBwKbf-ZzOf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_vegan_grouped = [word for comments in tokenized_vegan_grouped for word in comments if len(word) > 3]\n","tokenized_nonvegan_grouped = [word for comments in tokenized_nonvegan_grouped for word in comments if len(word) > 3]\n","tokenized_control_grouped = [word for comments in tokenized_control_grouped for word in comments if len(word) > 3]"],"metadata":{"id":"iI2doi6khjWA"},"id":"iI2doi6khjWA","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_vegan_grouped = [item for item in tokenized_vegan_grouped if not item.isdigit()]\n","tokenized_nonvegan_grouped = [item for item in tokenized_nonvegan_grouped if not item.isdigit()]\n","tokenized_control_grouped = [item for item in tokenized_control_grouped if not item.isdigit()]"],"metadata":{"id":"xlFPAMG4IC5Q"},"id":"xlFPAMG4IC5Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_vegan = max(vegan_grouped, key=len)\n","text_nonvegan = max(nonvegan_grouped, key=len)\n","text_control = max(control_grouped, key=len)"],"metadata":{"id":"79-E2-TYKO01"},"id":"79-E2-TYKO01","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","wordcloud = WordCloud().generate(text_vegan)\n","\n","import matplotlib.pyplot as plt\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","\n","# lower max_font_size\n","wordcloud = WordCloud(max_font_size=40).generate(text_vegan)\n","plt.figure()\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"SPznlvIBK53Z"},"id":"SPznlvIBK53Z","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","wordcloud = WordCloud().generate(text_nonvegan)\n","\n","import matplotlib.pyplot as plt\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","\n","# lower max_font_size\n","wordcloud = WordCloud(max_font_size=40).generate(text_nonvegan)\n","plt.figure()\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"YybfwjcRSFA9"},"id":"YybfwjcRSFA9","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","\n","wordcloud = WordCloud().generate(text_control)\n","\n","import matplotlib.pyplot as plt\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","\n","# lower max_font_size\n","wordcloud = WordCloud(max_font_size=40).generate(text_control)\n","plt.figure()\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"P3RRno3USFcm"},"id":"P3RRno3USFcm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","def create_large_wordcloud(words, filename=\"wordcloud.png\", max_words=200, width=1600, height=800, background_color=\"white\"):\n","    word_counts = Counter(words)\n","\n","    # 2. Create the WordCloud object\n","    wordcloud = WordCloud(\n","        max_words=max_words,\n","        width=width,\n","        height=height,\n","        background_color=background_color,\n","        # contour_width=3,  # Optional: adds a contour around the words\n","        # contour_color='steelblue', # Optional: sets the color of the contour\n","        # font_path='/path/to/your/font.ttf' # Optional: if you need a specific font\n","    ).generate_from_frequencies(word_counts)  # Use frequencies\n","\n","    # 3. Display the generated image:\n","    plt.figure(figsize=(width/100, height/100), dpi=100) # Adjust figure size for large images\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","\n","    # 4. Save the image:\n","    plt.savefig(filename, dpi=300, bbox_inches='tight')  # Save at high DPI\n","\n","    plt.show()\n","create_large_wordcloud(tokenized_vegan_grouped, filename=\"vegan_large_wordcloud.png\")"],"metadata":{"id":"1RAvWQesfK3E"},"id":"1RAvWQesfK3E","execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_large_wordcloud(tokenized_nonvegan_grouped, \"nonvegan_large_wordcloud.png\")"],"metadata":{"id":"vwQlA8UWrQaL"},"id":"vwQlA8UWrQaL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["create_large_wordcloud(tokenized_control_grouped, \"control_large_wordcloud.png\")"],"metadata":{"id":"RYEB4Nc8roxF"},"id":"RYEB4Nc8roxF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Codes below create semantic network where two nodes have edge if they are sequential in a comment. For this reason, we are going to use **ungrouped** version the comments."],"metadata":{"id":"3b87Bhcor0yL"},"id":"3b87Bhcor0yL"},{"cell_type":"code","source":["### Filter comments with less than 3 words\n","\n","vegan_network_list = [comment for comment in vegan_processed if len(comment.split()) > 2]\n","nonvegan_network_list = [comment for comment in nonVegan_processed if len(comment.split()) > 2]\n","control_network_list = [comment for comment in control_processed if len(comment.split()) > 2]"],"metadata":{"id":"MSiGMkclsrBt"},"id":"MSiGMkclsrBt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_short_words(comment):\n","    # Remove words of length 1 or 2\n","    return re.sub(r'\\b\\w{1,2}\\b', '', comment).strip()\n","\n","# Apply the function to all comments\n","vegan_network_list = [remove_short_words(comment) for comment in vegan_network_list]\n","nonvegan_network_list = [remove_short_words(comment) for comment in nonvegan_network_list]\n","control_network_list = [remove_short_words(comment) for comment in control_network_list]"],"metadata":{"id":"KOsZquWR_3nw"},"id":"KOsZquWR_3nw","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import networkx as nx\n","from collections import defaultdict\n","\n","G_vegan = nx.Graph()\n","vegan_edges = defaultdict(int)\n","\n","for comment in vegan_network_list:\n","# Build edges and track weights\n","  for word1, word2 in zip(comment.split(), comment.split()[1:]):\n","      # Increment weight for existing or new edge\n","      vegan_edges[(word1, word2)] += 1\n","\n","# Add weighted edges to the graph\n","G_vegan.add_weighted_edges_from([(w1, w2, weight) for (w1, w2), weight in vegan_edges.items()])"],"metadata":{"id":"GYFAcuNRa3vt"},"id":"GYFAcuNRa3vt","execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_nonvegan = nx.Graph()\n","nonvegan_edges = defaultdict(int)\n","\n","for comment in nonvegan_network_list:\n","# Build edges and track weights\n","  for word1, word2 in zip(comment.split(), comment.split()[1:]):\n","      # Increment weight for existing or new edge\n","      nonvegan_edges[(word1, word2)] += 1\n","\n","# Add weighted edges to the graph\n","G_nonvegan.add_weighted_edges_from([(w1, w2, weight) for (w1, w2), weight in nonvegan_edges.items()])"],"metadata":{"id":"tpwdbHp1t4fJ"},"id":"tpwdbHp1t4fJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["G_control = nx.Graph()\n","control_edges = defaultdict(int)\n","\n","for comment in control_network_list:\n","# Build edges and track weights\n","  for word1, word2 in zip(comment.split(), comment.split()[1:]):\n","      # Increment weight for existing or new edge\n","      control_edges[(word1, word2)] += 1\n","\n","# Add weighted edges to the graph\n","G_control.add_weighted_edges_from([(w1, w2, weight) for (w1, w2), weight in control_edges.items()])"],"metadata":{"id":"Ggft0Ciet_Ke"},"id":"Ggft0Ciet_Ke","execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 3\n","\n","vegan_filtered_edges = [(u, v, d) for u, v, d in G_vegan.edges(data=True) if d[\"weight\"] >= threshold]\n","Gvegan_filtered = nx.Graph()\n","Gvegan_filtered.add_edges_from(vegan_filtered_edges)"],"metadata":{"id":"kVxeB2KH5DqB"},"id":"kVxeB2KH5DqB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nonvegan_filtered_edges = [(u, v, d) for u, v, d in G_nonvegan.edges(data=True) if d[\"weight\"] >= threshold]\n","Gnonvegan_filtered = nx.Graph()\n","Gnonvegan_filtered.add_edges_from(nonvegan_filtered_edges)"],"metadata":{"id":"-_Opy2Ac5Dnr"},"id":"-_Opy2Ac5Dnr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["control_filtered_edges = [(u, v, d) for u, v, d in G_control.edges(data=True) if d[\"weight\"] >= threshold]\n","Gcontrol_filtered = nx.Graph()\n","Gcontrol_filtered.add_edges_from(control_filtered_edges)"],"metadata":{"id":"gxvRTKzM5DsP"},"id":"gxvRTKzM5DsP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["vegan_pr =  nx.pagerank(Gvegan_filtered)"],"metadata":{"id":"nY6pc6AT9SeG"},"id":"nY6pc6AT9SeG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["nonvegan_pr = nx.pagerank(Gnonvegan_filtered)"],"metadata":{"id":"_9H62rKR9SVc"},"id":"_9H62rKR9SVc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["control_pr = nx.pagerank(Gcontrol_filtered)"],"metadata":{"id":"Rp21AhsZfJe4"},"id":"Rp21AhsZfJe4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract top 20 words per dictionary\n","def get_top_words(pr_dict, n=20):\n","    return [word for word, _ in sorted(pr_dict.items(), key=lambda x: x[1], reverse=True)[:n]]\n","\n","# Create a DataFrame\n","top_20pr = pd.DataFrame({\n","    \"vegan\": get_top_words(vegan_pr),\n","    \"nonvegan\": get_top_words(nonvegan_pr),\n","    \"control\": get_top_words(control_pr)\n","})\n","\n","top_20pr\n"],"metadata":{"id":"JWc3bm8e9rmx"},"id":"JWc3bm8e9rmx","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}